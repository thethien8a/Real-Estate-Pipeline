# BDS - Batdongsan.com.vn Real Estate Data Scraper

## ğŸ“– MÃ´ táº£ dá»± Ã¡n

BDS lÃ  má»™t dá»± Ã¡n ETL (Extract, Transform, Load) pipeline Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng Python Ä‘á»ƒ thu tháº­p dá»¯ liá»‡u báº¥t Ä‘á»™ng sáº£n tá»« website lá»›n nháº¥t Viá»‡t Nam: [batdongsan.com.vn](https://batdongsan.com.vn). Dá»± Ã¡n táº­p trung vÃ o viá»‡c scrape dá»¯ liá»‡u listings (cÄƒn há»™, nhÃ  Ä‘áº¥t, dá»± Ã¡n) má»™t cÃ¡ch tá»± Ä‘á»™ng, chá»‘ng phÃ¡t hiá»‡n bot, vÃ  lÆ°u trá»¯ dÆ°á»›i dáº¡ng CSV Ä‘á»ƒ phÃ¢n tÃ­ch thá»‹ trÆ°á»ng, nghiÃªn cá»©u Ä‘áº§u tÆ° hoáº·c business intelligence.

### ğŸ¯ Má»¥c tiÃªu chÃ­nh
- Thu tháº­p dá»¯ liá»‡u chi tiáº¿t tá»« hÃ ng nghÃ¬n bÃ i Ä‘Äƒng báº¥t Ä‘á»™ng sáº£n.
- Há»— trá»£ xá»­ lÃ½ Ä‘á»“ng thá»i (concurrent) Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™.
- TrÃ¡nh bá»‹ cháº·n bá»Ÿi anti-bot mechanisms cá»§a website.
- Cung cáº¥p dá»¯ liá»‡u cÃ³ cáº¥u trÃºc (structured) vá»›i hÆ¡n 25 trÆ°á»ng thÃ´ng tin.

### ğŸ“Š Dá»¯ liá»‡u Ä‘Æ°á»£c thu tháº­p
Má»—i bÃ i Ä‘Äƒng bao gá»“m:
- **ThÃ´ng tin cÆ¡ báº£n**: TiÃªu Ä‘á», Ä‘á»‹a chá»‰, giÃ¡, diá»‡n tÃ­ch.
- **Chi tiáº¿t**: HÆ°á»›ng nhÃ , ban cÃ´ng, phÃ¡p lÃ½, ná»™i tháº¥t, sá»‘ phÃ²ng ngá»§/táº¯m/táº§ng.
- **Dá»± Ã¡n**: TÃªn dá»± Ã¡n, tÃ¬nh tráº¡ng, chá»§ Ä‘áº§u tÆ°.
- **BÃ i Ä‘Äƒng**: MÃ£ tin, ngÃ y Ä‘Äƒng/háº¿t háº¡n, loáº¡i tin.
- **Metadata**: URL nguá»“n, thá»i gian crawl.

Dá»¯ liá»‡u Ä‘Æ°á»£c lÆ°u vÃ o `data/raw/` vá»›i Ä‘á»‹nh dáº¡ng timestamped CSV (vÃ­ dá»¥: `batdongsan_raw_20251028_045314.csv`).

## ğŸ› ï¸ CÃ´ng nghá»‡ sá»­ dá»¥ng
- **NgÃ´n ngá»¯**: Python 3.12
- **Web Scraping**: nodriver 0.47.0 (thÆ° viá»‡n chá»‘ng phÃ¡t hiá»‡n bot, successor cá»§a undetected-chromedriver)
- **Xá»­ lÃ½ dá»¯ liá»‡u**: pandas 2.3.3, numpy 2.3.4
- **Async**: asyncio (built-in) cho concurrent processing
- **Database**: Supabase (PostgREST API)
- **LÆ°u trá»¯**: CSV files + Supabase staging/silver layers
- **Logging**: Python logging module
- **CI/CD**: GitHub Actions (scheduled automation 4 láº§n/ngÃ y)
- **MÃ´i trÆ°á»ng**: Virtual environment (venv)

## ğŸš€ CÃ i Ä‘áº·t

### 1. Clone hoáº·c táº£i dá»± Ã¡n
```bash
# Giáº£ sá»­ báº¡n Ä‘Ã£ táº£i vá» thÆ° má»¥c BDS
cd C:\Users\Asus\Downloads\BDS
```

### 2. Táº¡o virtual environment
```bash
# Táº¡o venv
python -m venv venv

# KÃ­ch hoáº¡t (Windows)
venv\Scripts\activate
```

### 3. CÃ i Ä‘áº·t dependencies
```bash
pip install -r requirements\requirements.txt
```

## ğŸ“Š Tráº¡ng thÃ¡i hiá»‡n táº¡i

### Tiáº¿n Ä‘á»™ ETL Pipeline

| Module | Tráº¡ng thÃ¡i | Chi tiáº¿t |
|--------|-----------|---------|
| **Extract** | âœ… HoÃ n thÃ nh | Scraping dá»¯ liá»‡u tá»« batdongsan.com.vn hoáº¡t Ä‘á»™ng á»•n Ä‘á»‹nh vá»›i 25+ fields. ÄÆ°á»£c tá»± Ä‘á»™ng hÃ³a qua GitHub Actions (4 láº§n/ngÃ y) |
| **Transform** | ğŸ”„ Äang phÃ¡t triá»ƒn | Äang xÃ¢y dá»±ng logic chuyá»ƒn Ä‘á»•i: dedup, upsert, trÃ­ch xuáº¥t page number tá»« URL |
| **Load** | ğŸŸ¡ Äang hoÃ n thiá»‡n | Load sang staging (Supabase) + Silver layer - cáº§n test ká»¹ lÆ°á»¡ng |
| **Gold Layer** | â³ Káº¿ hoáº¡ch | Chuáº©n bá»‹ cho data warehouse layer |

### Dá»¯ liá»‡u má»›i nháº¥t
- **Dá»¯ liá»‡u cuá»‘i cÃ¹ng**: 07/11/2025 - 05:10 (láº§n cháº¡y sÃ¡ng)
- **Sá»‘ lÆ°á»£ng báº£n ghi**: ~80 listings/láº§n crawl
- **Thá»i gian lÆ°u trá»¯**: CSV timestamped trong `data/processed/`
- **Automation**: Cháº¡y tá»± Ä‘á»™ng 4 láº§n/ngÃ y (5h, 11h, 17h, 23h UTC+7)

### CÃ´ng viá»‡c Æ°u tiÃªn tiáº¿p theo
1. âœ… HoÃ n thiá»‡n transform logic (dedup + upsert)
2. ğŸ”„ Fix & test load staging module (nodriver compatibility issue)
3. â³ Implement load gold layer
4. â³ ThÃªm unit tests + integration tests
5. â³ Tá»‘i Æ°u concurrency settings

---

## ğŸ“ Sá»­ dá»¥ng

### Cháº¡y scraper chÃ­nh
```bash
# Di chuyá»ƒn Ä‘áº¿n module extract
cd src\extract

# Cháº¡y chÆ°Æ¡ng trÃ¬nh
python crawl.py
```

- **Output**: Dá»¯ liá»‡u sáº½ Ä‘Æ°á»£c lÆ°u tá»± Ä‘á»™ng vÃ o `data/raw/` vá»›i timestamp.
- **Thá»i gian**: TÃ¹y thuá»™c vÃ o range page (máº·c Ä‘á»‹nh 1-50), khoáº£ng 10-30 phÃºt cho 50 trang.

### Cáº¥u hÃ¬nh tÃ¹y chá»‰nh
Chá»‰nh sá»­a `src/extract/config.py`:
- `START_PAGE` vÃ  `END_PAGE`: Pháº¡m vi trang cáº§n crawl (vÃ­ dá»¥: 1-100).
- `PAGE_SEMAPHORE_LIMIT`: Sá»‘ main page Ä‘á»“ng thá»i (máº·c Ä‘á»‹nh 4).
- `SUBPAGE_SEMAPHORE_LIMIT`: Sá»‘ subpage Ä‘á»“ng thá»i (máº·c Ä‘á»‹nh 10).

VÃ­ dá»¥: Äá»ƒ crawl 100 trang Ä‘áº§u:
```python
END_PAGE = 100
```

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
BDS/
â”œâ”€â”€ data/                  # LÆ°u trá»¯ dá»¯ liá»‡u
â”‚   â”œâ”€â”€ raw/              # Dá»¯ liá»‡u thÃ´ (CSV tá»« scraper)
â”‚   â”œâ”€â”€ staging/          # Dá»¯ liá»‡u Ä‘Ã£ transform (placeholder)
â”‚   â””â”€â”€ warehouse/        # Dá»¯ liá»‡u cuá»‘i cÃ¹ng (placeholder)
â”œâ”€â”€ dags/                 # Airflow DAGs (náº¿u dÃ¹ng orchestration, placeholder)
â”œâ”€â”€ documentation/        # TÃ i liá»‡u
â”‚   â”œâ”€â”€ config/           # Cáº¥u hÃ¬nh docs
â”‚   â”œâ”€â”€ guides/           # HÆ°á»›ng dáº«n sá»­ dá»¥ng
â”‚   â””â”€â”€ schemas/          # Schema dá»¯ liá»‡u
â”œâ”€â”€ learning/             # TÃ i liá»‡u há»c táº­p/research
â”œâ”€â”€ logs/                 # Log files (tá»± Ä‘á»™ng táº¡o)
â”œâ”€â”€ notebooks/            # Jupyter notebooks cho analysis
â”‚   â””â”€â”€ test.ipynb        # Notebook test (vÃ­ dá»¥)
â”œâ”€â”€ requirements/         # Dependencies
â”‚   â””â”€â”€ requirements.txt  # Danh sÃ¡ch packages
â”œâ”€â”€ .github/              # GitHub automation
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ daily-etl.yml # Scheduled ETL job
â”œâ”€â”€ src/                  # Source code chÃ­nh
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract/          # Module extract dá»¯ liá»‡u
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py     # Cáº¥u hÃ¬nh crawling
â”‚   â”‚   â”œâ”€â”€ crawl.py      # Logic chÃ­nh scraper
â”‚   â”‚   â”œâ”€â”€ utils.py      # Utility functions
â”‚   â”‚   â””â”€â”€ README.md     # Docs cho extract module
â”‚   â”œâ”€â”€ load/             # Module load dá»¯ liá»‡u
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ load_staging.py   # Load sang staging layer
â”‚   â”‚   â”œâ”€â”€ load_silver.py    # Load sang silver layer
â”‚   â”‚   â”œâ”€â”€ load_gold.py      # Load sang gold layer (WIP)
â”‚   â”‚   â”œâ”€â”€ supabase_class.py # Supabase integration
â”‚   â”‚   â””â”€â”€ README.md         # Docs cho load module
â”‚   â”œâ”€â”€ transform/        # Module transform dá»¯ liá»‡u
â”‚   â”‚   â””â”€â”€ documentation/    # Transform logic documentation
â”œâ”€â”€ tests/                # Tests toÃ n dá»± Ã¡n (placeholder)
â”œâ”€â”€ venv/                 # Virtual environment
â”œâ”€â”€ LICENSE               # MIT License
â””â”€â”€ README.md             # TÃ i liá»‡u nÃ y
```

## âœ¨ TÃ­nh nÄƒng ná»•i báº­t

- **Concurrent Scraping**: Xá»­ lÃ½ Ä‘á»“ng thá»i 4 main pages + 10 subpages/page Ä‘á»ƒ tÄƒng tá»‘c.
- **Anti-Detection**: Sá»­ dá»¥ng nodriver vá»›i stealth options (navigator manipulation, random delays).
- **Error Resilience**: Retry mechanism (reload page lÃªn Ä‘áº¿n 3 láº§n), exception handling cho ProtocolException.
- **Structured Output**: Dá»¯ liá»‡u CSV vá»›i schema rÃµ rÃ ng, dá»… import vÃ o Excel/Pandas.
- **Configurable**: Dá»… dÃ ng thay Ä‘á»•i range, concurrency qua config.py.
- **Logging**: Theo dÃµi tiáº¿n trÃ¬nh vÃ  lá»—i chi tiáº¿t.


## ğŸ¤ Contributing

1. Fork dá»± Ã¡n.
2. Táº¡o branch: `git checkout -b feature/new-feature`.
3. Commit changes: `git commit -m 'Add new feature'`.
4. Push: `git push origin feature/new-feature`.
5. Táº¡o Pull Request.

### ThÃªm tÃ­nh nÄƒng gá»£i Ã½
- Implement transform/load modules.
- ThÃªm database storage (PostgreSQL/SQLite).
- TÃ­ch há»£p Airflow cho scheduling.
- Unit tests vá»›i pytest.

## ğŸ“„ License

Dá»± Ã¡n Ä‘Æ°á»£c phÃ¢n phá»‘i dÆ°á»›i [MIT License](LICENSE). Copyright (c) 2025 Nguyá»…n Tháº¿ Thiá»‡n.

## ğŸ“ LiÃªn há»‡

- **TÃ¡c giáº£**: Nguyá»…n Tháº¿ Thiá»‡n, Nguyá»…n Há»“ng Nhung
- **Email**: [thethien04.work@gmail.com]
- **Issues**: Táº¡o issue trÃªn GitHub náº¿u cÃ³ bug hoáº·c feature request.

Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng BDS! Náº¿u cáº§n há»— trá»£, hÃ£y comment hoáº·c liÃªn há»‡. ğŸš€
